{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNEflow basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing data\n",
    "\n",
    "### 1.1.from MNE epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use MNE-python, all you need is to provide your epochs file (or list of epoch files) to mneflow.produce_tfrecords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "mne.set_log_level(verbose='CRITICAL')\n",
    "from mne.datasets import multimodal\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import mneflow\n",
    "print(mneflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Epochs |  940 events (all good), -0.0998976 - 0.499488 sec, baseline -0.0998976 â€“ 0 sec, ~531.4 MB, data loaded,\n",
      " 'Visual Upper right': 117\n",
      " 'Visual Lower right': 129\n",
      " 'Visual Lower left': 115\n",
      " 'Visual Upper left': 133\n",
      " 'Somato right': 107\n",
      " 'Somato left': 118\n",
      " 'Auditory right': 104\n",
      " 'Auditory left': 117>\n"
     ]
    }
   ],
   "source": [
    "fname_raw = os.path.join(multimodal.data_path(), 'multimodal_raw.fif')\n",
    "raw = mne.io.read_raw_fif(fname_raw)\n",
    "\n",
    "cond = raw.acqparser.get_condition(raw, None)\n",
    "# get the list of condition names\n",
    "condition_names = [k for c in cond for k,v in c['event_id'].items()]\n",
    "epochs_list = [mne.Epochs(raw, **c) for c in cond]\n",
    "epochs = mne.concatenate_epochs(epochs_list)\n",
    "epochs = epochs.pick_types(meg='grad')\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert epochs to TFRecord format\n",
    "See `mneflow.MetaData`  and `mneflow.produce_tfrecords` docstrings for description of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epochs\n",
      "Input shapes: X (n, ch, t) :  (940, 204, 361) y (n, [signal_channels], y_shape) :  (940, 1) \n",
      " input_type :  trials target_type :  int segment_y :  False\n",
      "Preprocessing:\n",
      "Scaling to interval 0.0 - 60.0\n",
      "n: 940\n",
      "Splitting into: 6 folds x 156\n",
      "Preprocessed: (940, 1, 301, 204) (940, 1) folds: 6 x 156\n",
      "Preprocessed targets:  (940, 1)\n",
      "Prepocessed sample shape: (1, 301, 204)\n",
      "Target shape actual/metadata:  (8,) (8,)\n",
      "Saving TFRecord# 0\n",
      "Updating: meta.data\n",
      "Updating: meta.preprocessing\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\data\\\\'\n",
    "data_id = 'mne_sample_multimodal'\n",
    "\n",
    "#Specify import options. \n",
    "\n",
    "import_opt = dict(path=path, #renamed from 'savepath'\n",
    "                  data_id=data_id, #renamed from 'out_name'\n",
    "                  input_type='trials',\n",
    "                  target_type='int',\n",
    "                  n_folds= 5,\n",
    "                  test_set = 'holdout',\n",
    "                  fs=600,\n",
    "                  overwrite=True,\n",
    "                  picks={'meg':'grad'},\n",
    "                  scale=True,  # apply baseline_scaling\n",
    "                  crop_baseline=True,  # remove baseline interval after scaling\n",
    "                  decimate=None,\n",
    "                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval\n",
    "                  )\n",
    "\n",
    "#write TFRecord files and metadata file\n",
    "meta = mneflow.produce_tfrecords(epochs, **import_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other import options\n",
    "### 1.2 Saved mne.epochs (*-epo.fif) files\n",
    "Alternatively, if your epochs are saved to disk provide a str (or list of str) with path(s) to your -epo.fif files\n",
    "\n",
    "e.g. this will work\n",
    "\n",
    "```python\n",
    "epochs.save('test_saved_epochs.fif')\n",
    "meta = mneflow.produce_tfrecords('test_saved_epochs.fif',**opt)\n",
    "```\n",
    "### 1.3. Arrays in *.mat or *.npz format\n",
    "if the first argument is str mneflow.produce_tfrecords can also accept *.mat or *.npz format\n",
    "\n",
    "e.g.\n",
    "\n",
    "```python\n",
    "data_path = '.../data_path/'\n",
    "filenames = [data_path +'sub' + str(i) + '-grad.npz' for i in range(1,4)]\n",
    "meta = mneflow.produce_tfrecords(filenames,**opt)\n",
    "```\n",
    "In this case, specify iput_type='array', and also provide array_keys keyword argument\n",
    "\n",
    "e.g. \n",
    "\n",
    "```python\n",
    "array_keys={'X':'my_data_samples','y':'my_labels'}\n",
    "```\n",
    "\n",
    "### 1.4. Tuple of (data, labels)\n",
    "Finally, if you have a more complex preprocessing pipeline, you can feed you data and labels as a tuple of arrays\n",
    "\n",
    "```python\n",
    "X = epochs.get_data()\n",
    "y = epochs.events[:,2]\n",
    "meta = mneflow.produce_tfrecords((X,y),**opt)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize the dataset object using the generated metadata file\n",
    "\n",
    "The dataset object includes several methods that allow experimenting with the dataset without the need to repeat the preprocessing or overwriting the TFRecord files each time.\n",
    "\n",
    "For example, you can train the model using any subset of classes, channels, or reduce the sampling rate by decimating across the time domain.\n",
    "\n",
    "See `mneflow.Dataset` docstring for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class_subset with 7 classes:\n",
      "Subset ratio 0.88, Multiplier 1.14\n",
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "Updating: meta.data\n"
     ]
    }
   ],
   "source": [
    "dataset = mneflow.Dataset(meta, train_batch=100, class_subset=[0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Choose from already implemented models\n",
    "\n",
    "MNEflow pipeline consists of three major parts:\n",
    "1. dataset\n",
    "2. computational graph\n",
    "\n",
    "Each part has its own set of hyper-parameters and methods that can be tuned. See help for mneflow.Dataset\n",
    "and mneflow.models.BaseModel for more details.\n",
    "In this example will we use LF-CNN network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating: meta.model_specs\n",
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "Updating: meta.data\n",
      "Setting reg for dmx, to l1\n",
      "Built: dmx input: (None, 1, 301, 204)\n",
      "Setting reg for tconv, to l1\n",
      "Built: tconv input: (None, 1, 301, 32)\n",
      "Setting reg for fc, to l1\n",
      "Built: fc input: (None, 1, 61, 32)\n",
      "Input shape: (1, 301, 204)\n",
      "y_pred: (None, 7)\n",
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# specify model parameters\n",
    "lfcnn_params = dict(n_latent=32, #number of latent factors\n",
    "                  filter_length=17, #convolutional filter length in time samples\n",
    "                  nonlin = tf.nn.relu,\n",
    "                  padding = 'SAME',\n",
    "                  pooling = 5,#pooling factor\n",
    "                  stride = 5, #stride parameter for pooling layer\n",
    "                  pool_type='max',\n",
    "                  dropout = .5,\n",
    "                  l1_scope = [\"weights\"],\n",
    "                  l1=3e-3)\n",
    "\n",
    "meta.update(model_specs=lfcnn_params)\n",
    "\n",
    "\"\"\"Initialize model\"\"\"\n",
    "model = mneflow.models.LFCNN(meta)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and (cross-)validation modes\n",
    "\n",
    "When training a model it is often useful to keep track of both: cross-validation and test-set performance.\n",
    "\n",
    "Default training mode is 'single_fold'\n",
    "Other training modes include n_fold cross-validation (mode='cv') and leave-one-subject-out cross-validation (mode='loso'). \n",
    "\n",
    "In 'cv' (cross-validation) mode with n_folds=5 and designated 'holdout' test set, test set performance is evaluated for each training fold and then averaged.\n",
    "\n",
    "In 'loso' mode each input tfrecord file is treated as a fold. Thus, if the data from each of n subjects is saved in a spearate .tfrecord file, on each fold the model will be trained on n - 1 subjects and then tested on the held out subject. In this setting each 'validation' fold comprises combined data from all (n - 1) subjects, but not the held out subject.\n",
    "\n",
    "See `mneflow.models.BaseModel.train` docstring for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating: meta.train_params\n",
      "Class weights:  None\n",
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "Epoch 1/20\n",
      "50/50 - 5s - loss: 2.2868 - cat_ACC: 0.2262 - val_loss: 1.9014 - val_cat_ACC: 0.3853 - 5s/epoch - 92ms/step\n",
      "Epoch 2/20\n",
      "50/50 - 4s - loss: 1.7601 - cat_ACC: 0.4502 - val_loss: 1.5109 - val_cat_ACC: 0.6606 - 4s/epoch - 78ms/step\n",
      "Epoch 3/20\n",
      "50/50 - 4s - loss: 1.2576 - cat_ACC: 0.6834 - val_loss: 1.1294 - val_cat_ACC: 0.8165 - 4s/epoch - 78ms/step\n",
      "Epoch 4/20\n",
      "50/50 - 4s - loss: 0.8858 - cat_ACC: 0.8179 - val_loss: 0.8813 - val_cat_ACC: 0.8532 - 4s/epoch - 79ms/step\n",
      "Epoch 5/20\n",
      "50/50 - 4s - loss: 0.6648 - cat_ACC: 0.8926 - val_loss: 0.7658 - val_cat_ACC: 0.8807 - 4s/epoch - 78ms/step\n",
      "Epoch 6/20\n",
      "50/50 - 4s - loss: 0.5360 - cat_ACC: 0.9330 - val_loss: 0.6990 - val_cat_ACC: 0.9083 - 4s/epoch - 81ms/step\n",
      "Epoch 7/20\n",
      "50/50 - 4s - loss: 0.4521 - cat_ACC: 0.9589 - val_loss: 0.6786 - val_cat_ACC: 0.8899 - 4s/epoch - 75ms/step\n",
      "Epoch 8/20\n",
      "50/50 - 4s - loss: 0.4000 - cat_ACC: 0.9768 - val_loss: 0.6638 - val_cat_ACC: 0.9083 - 4s/epoch - 75ms/step\n",
      "Epoch 9/20\n",
      "50/50 - 4s - loss: 0.3728 - cat_ACC: 0.9800 - val_loss: 0.6511 - val_cat_ACC: 0.8991 - 4s/epoch - 75ms/step\n",
      "Epoch 10/20\n",
      "50/50 - 4s - loss: 0.3557 - cat_ACC: 0.9823 - val_loss: 0.6434 - val_cat_ACC: 0.8991 - 4s/epoch - 79ms/step\n",
      "Epoch 11/20\n",
      "50/50 - 4s - loss: 0.3334 - cat_ACC: 0.9923 - val_loss: 0.6458 - val_cat_ACC: 0.9083 - 4s/epoch - 74ms/step\n",
      "Epoch 12/20\n",
      "50/50 - 4s - loss: 0.3257 - cat_ACC: 0.9913 - val_loss: 0.6361 - val_cat_ACC: 0.9083 - 4s/epoch - 79ms/step\n",
      "Epoch 13/20\n",
      "50/50 - 4s - loss: 0.3145 - cat_ACC: 0.9943 - val_loss: 0.6406 - val_cat_ACC: 0.8991 - 4s/epoch - 79ms/step\n",
      "Epoch 14/20\n",
      "50/50 - 4s - loss: 0.3036 - cat_ACC: 0.9953 - val_loss: 0.6548 - val_cat_ACC: 0.9083 - 4s/epoch - 80ms/step\n",
      "Epoch 15/20\n",
      "50/50 - 4s - loss: 0.2978 - cat_ACC: 0.9972 - val_loss: 0.6325 - val_cat_ACC: 0.8991 - 4s/epoch - 80ms/step\n",
      "Epoch 16/20\n",
      "50/50 - 4s - loss: 0.2951 - cat_ACC: 0.9955 - val_loss: 0.6312 - val_cat_ACC: 0.9174 - 4s/epoch - 79ms/step\n",
      "Epoch 17/20\n",
      "50/50 - 4s - loss: 0.2866 - cat_ACC: 0.9981 - val_loss: 0.6249 - val_cat_ACC: 0.9083 - 4s/epoch - 79ms/step\n",
      "Epoch 18/20\n",
      "50/50 - 4s - loss: 0.2833 - cat_ACC: 0.9972 - val_loss: 0.6310 - val_cat_ACC: 0.9083 - 4s/epoch - 78ms/step\n",
      "Epoch 19/20\n",
      "50/50 - 4s - loss: 0.2779 - cat_ACC: 0.9985 - val_loss: 0.6306 - val_cat_ACC: 0.9083 - 4s/epoch - 80ms/step\n",
      "Epoch 20/20\n",
      "50/50 - 4s - loss: 0.2724 - cat_ACC: 0.9985 - val_loss: 0.6275 - val_cat_ACC: 0.9083 - 4s/epoch - 79ms/step\n",
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "single_fold with 1 fold(s) completed. \n",
      "\n",
      "              Validation Performance: \n",
      "              Loss: 0.6249 +/- 0.0000.\n",
      "              Metric: 0.9083 +/- 0.0000\n",
      "\n",
      "\n",
      "              Test Performance: \n",
      "              Loss: 0.8290 +/- 0.0000.\n",
      "              Metric: 0.7895 +/- 0.0000\n",
      "Saving updated log to:  C:\\data\\models\\lfcnn_log.csv\n"
     ]
    }
   ],
   "source": [
    "# train the model for 20 epochs and stop the training if validation loss does not decrease during 5 consequtive epochs. \n",
    "\n",
    "model.train(n_epochs=20, eval_step=50, early_stopping=3, mode='single_fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class_subset with 7 classes:\n",
      "Subset ratio 1.00, Multiplier 1.00\n",
      "Test set: Loss = 0.8290 Accuracy = 0.7895\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(meta.data['test_paths'])\n",
    "print(\"Test set: Loss = {:.4f} Accuracy = {:.4f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model and the associated metadata for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving MetaData as mne_sample_multimodal_meta.pkl \n",
      "\n",
      "                  to C:\\data\\\n",
      "Updating: meta.data\n",
      "Updating: meta.train_params\n",
      "Updating: meta.model_specs\n",
      "Updating: meta.patterns\n",
      "Saving MetaData as mne_sample_multimodal_meta.pkl \n",
      "\n",
      "                  to C:\\data\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<mneflow.layers.Dense object at 0x00000290F50AA2E0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'mneflow.layers.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "meta.save()\n",
    "model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
