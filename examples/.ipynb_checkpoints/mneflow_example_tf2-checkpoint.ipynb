{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNEflow basic calssification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing data\n",
    "\n",
    "### 1.1.from MNE epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use MNE-python, all you need is to provide your epochs file (or list of epoch files) to mneflow.produce_tfrecords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/m/nbe/project/rtmeg/problearn/mneflow/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.datasets import multimodal\n",
    "import mneflow\n",
    "mne.set_log_level(verbose='CRITICAL')\n",
    "\n",
    "fname_raw = os.path.join(multimodal.data_path(), 'multimodal_raw.fif')\n",
    "raw = mne.io.read_raw_fif(fname_raw)\n",
    "\n",
    "cond = raw.acqparser.get_condition(raw, None)\n",
    "# get the list of condition names\n",
    "condition_names = [k for c in cond for k,v in c['event_id'].items()]\n",
    "epochs_list = [mne.Epochs(raw, **c) for c in cond]\n",
    "epochs = mne.concatenate_epochs(epochs_list)\n",
    "epochs = epochs.pick_types(meg='grad')\n",
    "print(epochs.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert epochs to TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epochs\n",
      "input shapes: X- (940, 204, 361) targets- (940,)\n",
      "training set: X- (799, 204, 302)  y- (799, 8)\n",
      "validation set: X- (141, 204, 302)  y- (141, 8)\n",
      "train preprocessed: (799, 1, 302, 204) (799, 8)\n",
      "Prepocessed sample shape: (1, 302, 204)\n",
      "Target shape actual/metadata:  (8,) (8,)\n",
      "Saving TFRecord# 0\n"
     ]
    }
   ],
   "source": [
    "#Specify import options\n",
    "import_opt = dict(savepath='/m/nbe/project/rtmeg/problearn/mneflow/tfr/',  # path where TFR files will be saved\n",
    "                  out_name='mne_sample_epochs',  # name of TFRecords files\n",
    "                  fs=600,\n",
    "                  input_type='trials',\n",
    "                  target_type='int',\n",
    "                  picks={'meg':'grad'},\n",
    "                  scale=True,  # apply baseline_scaling\n",
    "                  crop_baseline=True,  # remove baseline interval after scaling\n",
    "                  decimate=None,\n",
    "                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval\n",
    "                  val_size=0.15,  # validation set size set to 15% of all data\n",
    "                  overwrite=True,\n",
    "                  segment=False,\n",
    "                  test_set='holdout')\n",
    "\n",
    "\n",
    "#write TFRecord files and metadata file to disk\n",
    "meta = mneflow.produce_tfrecords([epochs], **import_opt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other import options\n",
    "### 1.2 Saved mne.epochs (*-epo.fif) files\n",
    "Alternatively, if your epochs are saved to disk provide a str (or list of str) with path(s) to your -epo.fif files\n",
    "\n",
    "e.g. this will work\n",
    "\n",
    "```python\n",
    "epochs.save('test_saved_epochs.fif')\n",
    "meta = mneflow.produce_tfrecords('test_saved_epochs.fif',**opt)\n",
    "```\n",
    "### 1.3. Arrays in *.mat or *.npz format\n",
    "if the first argument is str mneflow.produce_tfrecords can also accept *.mat or *.npz format\n",
    "\n",
    "e.g.\n",
    "\n",
    "```python\n",
    "data_path = '/m/nbe/scratch/braindata/izbrv/detection_data/'\n",
    "filenames = [data_path +'sub' + str(i) + '-grad.npz' for i in range(1,4)]\n",
    "meta = mneflow.produce_tfrecords(filenames,**opt)\n",
    "```\n",
    "In this case, specify iput_type='array', and also provide array_keys keyword argument\n",
    "\n",
    "e.g. \n",
    "\n",
    "```python\n",
    "array_keys={'X':'my_data_samples','y':'my_labels'}\n",
    "```\n",
    "\n",
    "### 1.4. Tuple of (data, labels)\n",
    "Finally, if you have a more complex preprocessing pipeline, you can feed you data and labels as a tuple of arrays\n",
    "\n",
    "```python\n",
    "X = epochs.get_data()\n",
    "y = epochs.events[:,2]\n",
    "meta = mneflow.produce_tfrecords((X,y),**opt)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize the dataset object using the generated metadata file\n",
    "\n",
    "The dataset object includes several methods that allow experimenting with the dataset without the need to repeat the preprocessing or overwriting the TFRecord files each time.\n",
    "\n",
    "For example, you can train the model using any subset of classes, channels or reduce the sampling rate by decimating across the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds batch size: 100\n",
      "ds batch size: 142\n",
      "setting batch size to default (100)\n",
      "ds batch size: 100\n"
     ]
    }
   ],
   "source": [
    "dataset = mneflow.Dataset(meta, train_batch=100, class_subset=None, pick_channels=None, decim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Choose from already implemented models\n",
    "\n",
    "MNEflow pipeline consists of three major parts:\n",
    "1. dataset\n",
    "2. computational graph\n",
    "\n",
    "Each part has its own set of hyper-parameters and methods that can be tuned. See help for mneflow.Dataset\n",
    "and mneflow.keras_models.Model for more details.\n",
    "In this example will we use LF-CNN network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built: demix input: (None, 1, 302, 204)\n",
      "demix : output : (None, 1, 302, 32)\n",
      "Built: lf_conv input: (None, 1, 302, 32)\n",
      "lf_conv : output : (None, 1, 302, 32)\n",
      "max_pool : output : (None, 1, 61, 32)\n",
      "fc :::\n",
      "Built: fc input: (None, 1, 61, 32)\n",
      "fc : output : (None, 8)\n",
      "Input shape: (1, 302, 204)\n",
      "y_pred: (None, 8)\n",
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# specify model parameters\n",
    "lf_params = dict(n_latent=32, #number of latent factors\n",
    "                  filter_length=17, #convolutional filter length in time samples\n",
    "                  nonlin = tf.nn.relu,\n",
    "                  padding = 'SAME',\n",
    "                  pooling = 5,#pooling factor\n",
    "                  stride = 5, #stride parameter for pooling layer\n",
    "                  pool_type='max',\n",
    "                  model_path = import_opt['savepath'],\n",
    "                  dropout = .5,\n",
    "                  l1_scope = [\"weights\"],\n",
    "                  l1=3e-4)\n",
    "\n",
    "model = mneflow.models.LFCNN(dataset, lf_params)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 1 steps\n",
      "Epoch 1/3\n",
      "demix : output : (None, 1, 302, 32)\n",
      "lf_conv : output : (None, 1, 302, 32)\n",
      "max_pool : output : (None, 1, 61, 32)\n",
      "fc : output : (None, 8)\n",
      "demix : output : (None, 1, 302, 32)\n",
      "lf_conv : output : (None, 1, 302, 32)\n",
      "max_pool : output : (None, 1, 61, 32)\n",
      "fc : output : (None, 8)\n",
      " 48/100 [=============>................] - ETA: 9s - loss: 1.8079 - accuracy: 0.3429"
     ]
    }
   ],
   "source": [
    "#train the model for 10 epochs\n",
    "model.train(n_epochs=10, \n",
    "            eval_step=100, \n",
    "            early_stopping=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore the trained model parameters\n",
    "LFCNN allows exploring parameters learned from the data in terms of toporaphies and the spectral properties of the latent sources contributing to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_patterns(meta['val_paths'])\n",
    "model.plot_patterns('Vectorview-grad', sorting='weight_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = model.plot_spectra(sorting='weight_corr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
