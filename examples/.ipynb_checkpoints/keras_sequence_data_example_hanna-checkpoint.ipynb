{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Metadata file found, restoring\n",
      "ds batch size: 100\n",
      "ds batch size: 100\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "os.chdir('/m/nbe/project/rtmeg/problearn/mneflow')\n",
    "\n",
    "import mneflow\n",
    "from mneflow import models\n",
    "from mneflow import layers\n",
    "\n",
    "# from mneflow.keras_utils import plot_metrics, plot_output, plot_history_v2\n",
    "from mneflow.keras_utils import mse_weighted, mae_weighted\n",
    "from mneflow.keras_utils import r_square, soft_acc, rmse\n",
    "\n",
    "# Force enable eager execution after importing mneflow\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Dataset parameters\n",
    "dpath = '/m/nbe/scratch/strokemotor/healthysubjects/'\n",
    "# fname = 'sub1/short_epochs.fif'\n",
    "fnames = [''.join([dpath, 'sub', str(ii), '/long_epochs.fif']) for ii in range(2, 3)]\n",
    "\n",
    "import_opt = dict(fs=1000,\n",
    "                  savepath='/m/nbe/scratch/braindata/izbrv/strokemotor/healthy/tfr/',\n",
    "                  out_name='stm-5x250-a125-seq',\n",
    "                  input_type='seq',\n",
    "                  overwrite=False,\n",
    "                  val_size=0.2,\n",
    "                  # array_keys={'X': 'train_data', 'y': 'train_dg'},\n",
    "                  # picks=np.arange(0,306,3),\n",
    "                  bp_filter=(.1, 125),\n",
    "                  # target_picks=None,\n",
    "                  target_type='int',\n",
    "                  segment=250,\n",
    "                  augment=True,\n",
    "                  aug_stride=100,\n",
    "                  # transpose=('X', 'y'),\n",
    "                  combine_events={3: 0, 4: 1, 5: 1, 6: 0, 2: 2},\n",
    "                  scale=True,\n",
    "                  scale_interval=(0, 1000),\n",
    "                  decimate=None,\n",
    "                  # transform_targets=False,\n",
    "                  seq_length=17,\n",
    "                  test_set='holdout'\n",
    "                  )\n",
    "\n",
    "meta = mneflow.produce_tfrecords(fnames, **import_opt)\n",
    "\n",
    "nbatch = 100\n",
    "steps = 100\n",
    "\n",
    "# batch the dataset according to that value\n",
    "dataset = mneflow.Dataset(meta, train_batch=nbatch, class_subset=None,\n",
    "                          pick_channels=None, decim=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "/m/nbe/project/rtmeg/problearn/mneflow/mneflow/__init__.py\n",
      "tf version: 2.1.0\n",
      "executing eagerly: True\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "print('---------\\n'+mneflow.__file__)\n",
    "print('tf version: '+tf.__version__)\n",
    "print('executing eagerly: '+str(tf.executing_eagerly())+'\\n---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-mix init : OK\n",
      "fc init : OK\n",
      "de-mix init : OK\n",
      "lstm init : OK\n",
      "fc init : OK\n"
     ]
    }
   ],
   "source": [
    "# specify optimizer parmeters\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "# specify model parameters\n",
    "graph_specs = dict(n_ls=32,  # number of latent factors\n",
    "                   filter_length=18,  # convolutional filter length\n",
    "                   pooling=5,  # convlayer pooling factor\n",
    "                   stride=5,  # stride parameter for pooling layer\n",
    "                   padding='SAME',\n",
    "                   dropout=.5,\n",
    "                   nonlin=tf.nn.relu,\n",
    "                   pool_type='max',\n",
    "                   out_dim=np.prod(meta['y_shape']),\n",
    "                   axis=2,\n",
    "                   y_shape=meta['y_shape'],\n",
    "                   model_path=import_opt['savepath'],  # not used at the moment\n",
    "                   # regularization parameters\n",
    "                   l1=3e-4,\n",
    "                   l2=3e-3,\n",
    "                   # LSTM parameters\n",
    "                   rnn_units=np.prod(meta['y_shape']),\n",
    "                   rnn_dropout=0.0,\n",
    "                   rnn_nonlin='elu',\n",
    "                   rnn_rec_nonlin='tanh',\n",
    "                   rnn_forget_bias=False,\n",
    "                   rnn_seq=False,\n",
    "                   unroll=False)\n",
    "\n",
    "model = models.LFLSTM(graph_specs, dataset)\n",
    "loss_f = tf.compat.v1.losses.softmax_cross_entropy  # one-hot labels for classification\n",
    "\n",
    "# % builtin\n",
    "model.compile(loss=loss_f, optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 5, 204, 250]\n",
      "input x0 (?, 5, 204, 250)\n",
      "input x0 (?, 5, 204, 250)\n",
      "de-mix built : OK\n",
      "dmx (?, 5, 250, 32)\n",
      "demix dmx (?, 5, 250, 32)\n",
      "dmx-sqout: (?, 250, 32, 1)\n",
      "conv build : OK\n",
      "tconv (?, 250, 32, 1)\n",
      "pooled (?, 2, 32, 1)\n",
      "flat features: (?, 5, 64)\n",
      "lstm_out (?, 32)\n",
      "fc ::: 32 3\n",
      "fc build : OK\n",
      "fc y_ (?, 3)\n",
      "Train for 100 steps, validate for 1 steps\n",
      "Epoch 1/30\n",
      "[None, 5, 204, 250]\n",
      "input x0 (?, 5, 204, 250)\n",
      "input x0 (?, 5, 204, 250)\n",
      "dmx (?, 5, 250, 32)\n",
      "demix dmx (?, 5, 250, 32)\n",
      "dmx-sqout: (?, 250, 32, 1)\n",
      "tconv (?, 250, 32, 1)\n",
      "pooled (?, 2, 32, 1)\n",
      "flat features: (?, 5, 64)\n",
      "lstm_out (?, 32)\n",
      "fc y_ (?, 3)\n",
      "[None, 5, 204, 250]\n",
      "input x0 (?, 5, 204, 250)\n",
      "input x0 (?, 5, 204, 250)\n",
      "demix dmx (?, 5, 250, 32)\n",
      "dmx-sqout: (?, 250, 32, 1)\n",
      "tconv (?, 250, 32, 1)\n",
      "pooled (?, 2, 32, 1)\n",
      "flat features: (?, 5, 64)\n",
      "lstm_out (?, 32)\n",
      "fc y_ (?, 3)\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.6549 - acc: 0.3414[None, 5, 204, 250]\n",
      "input x0 (?, 5, 204, 250)\n",
      "input x0 (?, 5, 204, 250)\n",
      "dmx (?, 5, 250, 32)\n",
      "demix dmx (?, 5, 250, 32)\n",
      "dmx-sqout: (?, 250, 32, 1)\n",
      "tconv (?, 250, 32, 1)\n",
      "pooled (?, 2, 32, 1)\n",
      "flat features: (?, 5, 64)\n",
      "lstm_out (?, 32)\n",
      "fc y_ (?, 3)\n",
      "100/100 [==============================] - 67s 675ms/step - loss: 1.6545 - acc: 0.3408 - val_loss: 1.5377 - val_acc: 0.3900\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 66s 663ms/step - loss: 1.5295 - acc: 0.3904 - val_loss: 1.5192 - val_acc: 0.4100\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 66s 659ms/step - loss: 1.4626 - acc: 0.4388 - val_loss: 1.5060 - val_acc: 0.3200\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 66s 660ms/step - loss: 1.3580 - acc: 0.5164 - val_loss: 1.4686 - val_acc: 0.3200\n",
      "Epoch 5/30\n",
      " 48/100 [=============>................] - ETA: 34s - loss: 1.3386 - acc: 0.5200"
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=5e-6, patience=10)\n",
    "\n",
    "# %% train the model\n",
    "history = model.fit(dataset.train, validation_data=dataset.val, epochs=30,\n",
    "                    steps_per_epoch=steps, validation_steps=1,\n",
    "                    callbacks=[stop_early], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = keras_models.plot_cm(model, dataset, 'train', steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = keras_models.plot_cm(model, dataset, 'val', steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = keras_models.plot_cm(model, dataset, 'test', steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
